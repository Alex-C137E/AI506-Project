{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassicML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xq7alMFH8zsM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def make_binary_data(cookbook, ingredient_list):\n",
        "    # X is a binary matrix with recipe rows and ingredients columns, \n",
        "    # where (recipe, ingredients) is 1 if the ingredient is present in the recipe\n",
        "\n",
        "    n_recipies = len(cookbook)\n",
        "    n_ingredients = len(ingredient_list)\n",
        "\n",
        "    X = np.zeros((n_recipies, n_ingredients))\n",
        "    y = np.zeros(n_recipies)\n",
        "    for idx, recipe in enumerate(cookbook):\n",
        "        y[idx] = recipe['kitchen_id']\n",
        "        for ingredient_id in recipe['ingredients']:\n",
        "            X[idx][ingredient_id] = 1\n",
        "            \n",
        "    return X,y\n",
        "\n",
        "\n",
        "def make_recipe_embedding_data(cookbook, embedding, avg=False):\n",
        "    # Make the data. X is a matrix with recipe rows and embedding dimension columns, \n",
        "    # where every row is the average of the embeddings in the recipes\n",
        "\n",
        "    n_recipies = len(cookbook)\n",
        "    embedding_dim = len(embedding[0])\n",
        "\n",
        "    X = np.zeros((n_recipies, embedding_dim))\n",
        "    y = np.zeros(n_recipies)\n",
        "    for idx, recipe in enumerate(cookbook):\n",
        "        embedding_avg = np.zeros(embedding_dim) \n",
        "        ingredient_count = len(recipe['ingredients'])\n",
        "        y[idx] = recipe['kitchen_id']\n",
        "        for ingredient_id in recipe['ingredients']:\n",
        "            if avg:\n",
        "                embedding_avg += embedding[ingredient_id] / ingredient_count\n",
        "            else:\n",
        "                embedding_avg += embedding[ingredient_id]\n",
        "            \n",
        "        X[idx, :] = embedding_avg\n",
        "            \n",
        "    return X,y\n",
        "\n",
        "\n",
        "def make_ingredient_embedding_data(embedding):\n",
        "    # A matrix with as rows the ingredients and as columns the embedding\n",
        "    matrix = np.empty( (len(embedding), len(embedding[0])) )\n",
        "    for ingredient_idx, embedding_vector in embedding.items():\n",
        "        matrix[ingredient_idx, :] = embedding_vector\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "__kitchen_list = []\n",
        "\n",
        "\n",
        "def get_cookbook_train():\n",
        "\n",
        "    cookbook_train = []\n",
        "\n",
        "    with open('train.csv', 'r') as file:\n",
        "\n",
        "        csv_reader = reader(file, delimiter=\",\")\n",
        "\n",
        "        for i, row in enumerate(csv_reader):\n",
        "            \n",
        "            kitchen = row[-1]\n",
        "            ingredient_strings = row[:-1]\n",
        "            ingredients = [ int(s) for s in ingredient_strings ]\n",
        "            \n",
        "            cookbook_train.append({\n",
        "                'recipe_id': i,\n",
        "                'ingredients': ingredients,\n",
        "                'kitchen_name': kitchen\n",
        "            })      \n",
        "\n",
        "    fill_kitchen_list(cookbook_train)\n",
        "    add_kitchen_id_to_cookbook(cookbook_train)\n",
        "    return cookbook_train\n",
        "\n",
        "\n",
        "def get_cookbook_valid_question():\n",
        "\n",
        "    cookbook_valid_question = []\n",
        "\n",
        "    with open('validation_classification_question.csv', 'r') as file:\n",
        "\n",
        "        csv_reader = reader(file, delimiter=\",\")\n",
        "\n",
        "        for i, row in enumerate(csv_reader):\n",
        "            ingredients = [ int(s) for s in row ]\n",
        "            cookbook_valid_question.append({\n",
        "                'recipe_id': i,\n",
        "                'ingredients': ingredients,\n",
        "                'kitchen_name': \"UNKNOWNKITCHEN\",\n",
        "                'kitchen_id': \"-999\"\n",
        "            })     \n",
        "\n",
        "    return cookbook_valid_question\n",
        "\n",
        "\n",
        "       \n",
        "def get_cookbook_valid_answer():\n",
        "\n",
        "    cookbook_valid_answer = []\n",
        "\n",
        "    with open('validation_classification_answer.csv', 'r') as file:\n",
        "\n",
        "        csv_reader = reader(file, delimiter=\",\")\n",
        "\n",
        "        for i, row in enumerate(csv_reader):        \n",
        "            kitchen = row[0]        \n",
        "            cookbook_valid_answer.append({\n",
        "                'recipe_id': i,\n",
        "                'ingredients': [],\n",
        "                'kitchen_name': kitchen\n",
        "            })\n",
        "\n",
        "    fill_kitchen_list(cookbook_valid_answer)\n",
        "    add_kitchen_id_to_cookbook(cookbook_valid_answer)\n",
        "    return cookbook_valid_answer\n",
        "\n",
        "\n",
        "def get_ingredient_list():\n",
        "\n",
        "    node_ingredient = pd.read_fwf('node_ingredient.csv', header=None)\n",
        "    ingredient_list = [None] * node_ingredient.shape[0]\n",
        "    for index , row in node_ingredient.iterrows():\n",
        "        ingredient_list[index] = row[0]\n",
        "\n",
        "    return ingredient_list\n",
        "\n",
        "\n",
        "def fill_kitchen_list(cookbook):\n",
        "\n",
        "    for recipe in cookbook:\n",
        "        if recipe['kitchen_name'] not in __kitchen_list:\n",
        "            __kitchen_list.append(recipe['kitchen_name'])\n",
        "\n",
        "\n",
        "\n",
        "def add_kitchen_id_to_cookbook(cookbook):   \n",
        "\n",
        "    for recipe in cookbook:\n",
        "        kitchen_id = __kitchen_list.index(recipe['kitchen_name'])\n",
        "        recipe['kitchen_id'] = kitchen_id"
      ],
      "metadata": {
        "id": "pZ_ePD9c87q3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cookbook_train = get_cookbook_train()\n",
        "cookbook_valid_question = get_cookbook_valid_question()\n",
        "cookbook_valid_answer = get_cookbook_valid_answer()\n",
        "ingredient_list = get_ingredient_list()\n"
      ],
      "metadata": {
        "id": "6oNVEili9A1T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "V5XYTFjc9IC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binTest = make_binary_data(cookbook_train,ingredient_list)\n",
        "Xtrain = binTest[0]\n",
        "Xtrain = Xtrain.tolist()\n",
        "\n",
        "Ytrain = []\n",
        "for i in cookbook_train:\n",
        "  Ytrain.append(i[\"kitchen_name\"])\n",
        "\n",
        "binVal = make_binary_data(cookbook_valid_question,ingredient_list)\n",
        "Xval = binVal[0]\n",
        "Xval = Xval.tolist()\n",
        "\n",
        "Yval = []\n",
        "for i in cookbook_valid_answer:\n",
        "  Yval.append(i[\"kitchen_name\"])"
      ],
      "metadata": {
        "id": "egGqzWqD9E_t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest classifier"
      ],
      "metadata": {
        "id": "ZFUH-uOS9Q4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "clf = RandomForestClassifier(min_samples_leaf = 2)\n",
        "clf.fit(Xtrain, Ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfNwNmMn9KSy",
        "outputId": "abac8a1c-c356-4b07-9fbc-1f60f9dbeeb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(min_samples_leaf=2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for i in range(1000):\n",
        "  pred = clf.predict([Xval[i]])\n",
        "  if pred[0] == Yval[i]:\n",
        "    acc += 1\n",
        "print('accuracy RF:',acc/1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGVK-m-K9SuW",
        "outputId": "1bc024be-d50c-4b15-b278-ca8acbeb2813"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy RF: 0.623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf2 = KNeighborsClassifier(n_neighbors=3)\n",
        "clf2.fit(Xtrain, Ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb00llvx9Wre",
        "outputId": "b4584eae-369e-4dab-af7c-2a8a2b081e28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for i in range(1000):\n",
        "  pred = clf2.predict([Xval[i]])\n",
        "  if pred[0] == Yval[i]:\n",
        "    acc += 1\n",
        "print('accuracy 3nn:',acc/1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbTZZmhq9bGC",
        "outputId": "32f0b679-fd98-4698-da54-3ba9430bd1e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 3nn: 0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PCA to reduce dimensionality to train differents methods.\n",
        "\n",
        "Create new input space with 256 and 64 dimension"
      ],
      "metadata": {
        "id": "Aj-m0e-p9eqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca1 = PCA(n_components=256)\n",
        "pca1.fit(Xtrain)\n",
        "\n",
        "pca2 = PCA(n_components=64)\n",
        "pca2.fit(Xtrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XybfVke9btX",
        "outputId": "662b4824-ce7d-4ccb-d9be-a61c91fa6df2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=64)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XtrainPCA1 = Xtrain.copy()\n",
        "XvalPCA1 = Xval.copy()\n",
        "XtrainPCA1 = pca1.transform(XtrainPCA1)\n",
        "XvalPCA1 = pca1.transform(XvalPCA1)\n",
        "\n",
        "XtrainPCA2 = Xtrain.copy()\n",
        "XvalPCA2 = Xval.copy()\n",
        "XtrainPCA2 = pca2.transform(XtrainPCA2)\n",
        "XvalPCA2 = pca2.transform(XvalPCA2)"
      ],
      "metadata": {
        "id": "kB0z7qoo9n89"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support vector classifie"
      ],
      "metadata": {
        "id": "tQU0FvJ19slS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "clf3 = SVC()\n",
        "clf3.fit(XtrainPCA1, Ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFZI-wU99qbW",
        "outputId": "93e74a3e-9501-4f05-eea2-652fed782da2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for i in range(1000):\n",
        "  pred = clf3.predict([XvalPCA1[i]])\n",
        "  if pred[0] == Yval[i]:\n",
        "    acc += 1\n",
        "print('accuracy SVM PCA:', acc/1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9elbFY4C9vI2",
        "outputId": "040eec33-741e-4759-e922-506dfe7602a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy SVM PCA: 0.673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGboost"
      ],
      "metadata": {
        "id": "60W7bEtb92YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf4 = GradientBoostingClassifier(learning_rate=0.1,max_depth=7, random_state=0,verbose=1,min_samples_leaf = 20)\n",
        "clf4.fit(XtrainPCA2, Ytrain).score(XvalPCA2, Yval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsNIzKx19zjq",
        "outputId": "2e844db7-43ca-4b34-ad25-af59821cfd41"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           2.1641           45.04m\n",
            "         2           1.9616           44.49m\n",
            "         3           1.8102           44.04m\n",
            "         4           1.6895           43.62m\n",
            "         5           1.5873           43.14m\n",
            "         6           1.4971           42.63m\n",
            "         7           1.4190           42.16m\n",
            "         8           1.3516           41.71m\n",
            "         9           1.2886           41.23m\n",
            "        10           1.2328           40.77m\n",
            "        20           0.8456           36.10m\n",
            "        30           0.6241           31.55m\n",
            "        40           0.4735           27.01m\n",
            "        50           0.3675           22.51m\n",
            "        60           0.2898           18.02m\n",
            "        70           0.2305           13.52m\n",
            "        80           0.1874            9.02m\n",
            "        90           0.1528            4.51m\n",
            "       100           0.1256            0.00s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.574796126401631"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for i in range(1000):\n",
        "  pred = clf4.predict([XvalPCA2[i]])\n",
        "  if pred[0] == Yval[i]:\n",
        "    acc += 1\n",
        "print('accuracy XGboost PCA:', acc/1000)"
      ],
      "metadata": {
        "id": "bfQDPpx194m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN with reduce dimension"
      ],
      "metadata": {
        "id": "a2Me0wrm9-47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf5 = KNeighborsClassifier(n_neighbors=3)\n",
        "clf5.fit(XtrainPCA1, Ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXRrrvMb965c",
        "outputId": "9e047b69-5c12-4a6c-9f71-68c0aa6ea834"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for i in range(1000):\n",
        "  pred = clf5.predict([XvalPCA1[i]])\n",
        "  if pred[0] == Yval[i]:\n",
        "    acc += 1\n",
        "print('accuracy 3nn PCA:', acc/1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfoG2nkc9_uk",
        "outputId": "ff141687-36a1-4656-a2e5-c8714f2709de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 3nn PCA: 0.457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes classifier"
      ],
      "metadata": {
        "id": "96PF8EG4-GgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "clf6 = GaussianNB()\n",
        "clf6.fit(XtrainPCA1, Ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OziNMeA-EWp",
        "outputId": "2ae34548-32d4-4e67-bf3e-fd3ae84dbea0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for i in range(1000):\n",
        "  pred = clf6.predict([XvalPCA1[i]])\n",
        "  if pred[0] == Yval[i]:\n",
        "    acc += 1\n",
        "print('Gaussian PCA:', acc/1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhNdjmBo-Jzn",
        "outputId": "440536ef-12ef-43d2-ab78-837fb7e55025"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian PCA: 0.433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking -> RandomForest + SVC + knn + GB"
      ],
      "metadata": {
        "id": "P6WQwW-Y-PAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier()),\n",
        "    ('svc', SVC()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=3)),\n",
        "    ('bayes', GaussianNB())         \n",
        "]\n",
        "clfS = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(dual=False))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "clfS.fit(XtrainPCA2, Ytrain).score(XvalPCA2, Yval)"
      ],
      "metadata": {
        "id": "uh02QqxH-MRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = 0\n",
        "for i in range(1000):\n",
        "  pred = clfS.predict([XvalPCA1[i]])\n",
        "  if pred[0] == Yval[i]:\n",
        "    acc += 1\n",
        "print('Stacking PCA:', acc/1000)"
      ],
      "metadata": {
        "id": "1LxUuWWo-SP9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}